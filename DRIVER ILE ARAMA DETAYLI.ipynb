{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import os\n",
    "import undetected_chromedriver as uc\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "#driver\n",
    "class WebScrappingDriver:\n",
    "    def __init__(self):\n",
    "        self.driver = uc.Chrome(version_main=134)\n",
    "        self.driver.execute_cdp_cmd('Network.setCacheDisabled', {'cacheDisabled': True})\n",
    "\n",
    "    def get_page(self, link):\n",
    "        self.driver.delete_all_cookies()\n",
    "        self.driver.get(link)\n",
    "\n",
    "    def get_page_source_soup(self):\n",
    "        source = self.driver.page_source\n",
    "        soup = BeautifulSoup(source, \"html.parser\")\n",
    "        return soup\n",
    "\n",
    "    #NEW\n",
    "    def get_xpath_value(self, input_xpath_val):\n",
    "        return self.driver.find_element(By.XPATH, input_xpath_val)\n",
    "\n",
    "    def send_keys_to_text_box_by_name(self, id_name_par, text_to_send):\n",
    "        self.textbox = self.driver.find_element(By.NAME, id_name_par)\n",
    "        self.textbox.clear()\n",
    "        self.textbox.send_keys(text_to_send)\n",
    "        self.textbox.send_keys(Keys.ENTER)\n",
    "\n",
    "    def click_By_xpath(self,xpath_value):\n",
    "        self.driver.find_element(By.XPATH, xpath_value).click()\n",
    "\n",
    "    def click_By_id(self,id_value):\n",
    "        self.driver.find_element(By.ID, id_value).click()\n",
    "\n",
    "    def quit_driver(self):\n",
    "        self.driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame()\n",
    "result_df['Keyword'] = None\n",
    "result_df['Link'] = None\n",
    "result_df['Özel Durum'] = None #-y-s\n",
    "result_df['Özel Durum swA'] = None #-y-s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ozel_durumlar_dict = {\"-y-s\":\"SLP\", \"-p-\":\"Product\", \"-x-c\":\"Kategori\", \"-x-b\":\"Marka\"} #SLP, Product, özel durum, sonundaki 2 - ile ayrılan a ile ve v ile bitcek -> swA(daha yazılmadı), Kategori, Marka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kac_sf_aratilcak = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "magaza = \"trendyol\" #pazarama-trendyol\n",
    "ek_magaza = \" \" + magaza\n",
    "magaza_link = \"www.\" + magaza + \".com\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#TY SATICILARI ARATMAK İSTEDİĞİMDE; BUNU TÜM SERVERLARA KOPYALA\n",
    "kac_sf_aratilcak = 1\n",
    "\n",
    "magaza = \"\" \n",
    "ek_magaza = \"\" + magaza\n",
    "magaza_link = \"www.\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper = WebScrappingDriver()\n",
    "link = \"https://www.google.com/\"\n",
    "scraper.get_page(link)\n",
    "#-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Arama Sonucu Seçicileri\n",
    "def return_each_google_search_info(soup_par):\n",
    "    results_dict = {}\n",
    "    for result in soup_par.find_all(\"div\", class_=\"tF2Cxc\"):  # Google sonuçlarını seçer\n",
    "        try:\n",
    "            url = result.find(\"span\",{\"jsaction\":\"rcuQ6b:npT2md;PYDNKe:bLV6Bd;mLt3mc\"}).find(\"a\")['href'] # URL\n",
    "        except:\n",
    "            url = None\n",
    "\n",
    "        try:\n",
    "            title = result.find(\"h3\").text.strip() # Başlık\n",
    "        except:\n",
    "            title = \"Bulunamadı\"\n",
    "\n",
    "        try:\n",
    "            description = result.find(\"div\",{\"data-snf\":\"nke7rc\"}).find('span').text # Açıklama 1\n",
    "        except:\n",
    "            description = \"Bulunamadı\"\n",
    "\n",
    "        try:\n",
    "            description2 = result.find(\"div\",{\"data-snf\":\"mCCBcf\"}).text.replace(\"\\xa0\",\" \").replace(\"  \",\" \").strip() # Açıklama 2\n",
    "        except:\n",
    "            description2 = \"Bulunamadı\"\n",
    "        if url:\n",
    "            results_dict[url] = [title, description, description2]\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_excel = pd.read_excel(\"detaylı search input/\" + os.listdir(\"detaylı search input/\")[0])\n",
    "keywords_list = input_excel['Keywords'].values.tolist()\n",
    "#-\n",
    "result_df_sayac = 0\n",
    "#-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for kw_index, keyword in enumerate(keywords_list):\n",
    "    trendyol_links_main_dict = {}\n",
    "\n",
    "    key = 2\n",
    "    while(key):\n",
    "        try:\n",
    "            keyword_to_search = str(keyword).lower() + ek_magaza\n",
    "            scraper.send_keys_to_text_box_by_name('q', keyword_to_search) #aratıyor, enter denmesi lazım\n",
    "            time.sleep(random.randint(22,36))#time.sleep(random.randint(18,32))\n",
    "            #-\n",
    "            soup = scraper.get_page_source_soup()\n",
    "\n",
    "            all_search_tags = soup.find(\"div\",{\"id\":\"search\"})\n",
    "            \n",
    "            if(all_search_tags):\n",
    "                result_dict = return_each_google_search_info(soup)\n",
    "                if result_dict:\n",
    "                    trendyol_links_main_dict.update(result_dict)\n",
    "                #---\n",
    "\n",
    "                #Get Next Pages\n",
    "                for next_page_count in range(2, kac_sf_aratilcak+1):\n",
    "                    next_page_link = soup.find(\"a\",{\"aria-label\":\"Page \" + str(next_page_count)})\n",
    "                    if(next_page_link):\n",
    "                        next_page_link_full = \"https://www.google.com/\" + next_page_link['href']\n",
    "                        scraper.get_page(next_page_link_full)\n",
    "                        time.sleep(random.randint(22,36))#time.sleep(random.randint(18,32))\n",
    "\n",
    "                        soup = scraper.get_page_source_soup()\n",
    "                        all_search_tags = soup.find(\"div\",{\"id\":\"search\"})\n",
    "\n",
    "                        if(all_search_tags):\n",
    "                            result_dict = return_each_google_search_info(soup)\n",
    "                            if result_dict:\n",
    "                                trendyol_links_main_dict.update(result_dict)\n",
    "                key = 0\n",
    "        except:\n",
    "            if(key):\n",
    "                key -= 1\n",
    "                scraper.quit_driver()\n",
    "                time.sleep(5)\n",
    "                print(\"hata verdi 10 saniye bekleyip aç kapa yapacak!\")\n",
    "                scraper = WebScrappingDriver()\n",
    "                scraper.get_page(link)\n",
    "\n",
    "    if trendyol_links_main_dict:\n",
    "        for uniq_trendyol_link in trendyol_links_main_dict.keys():\n",
    "            if magaza_link in uniq_trendyol_link and \"text=\" not in uniq_trendyol_link: #trendyol linki ise\n",
    "                result_df.at[result_df_sayac, 'Keyword'] = keyword_to_search\n",
    "                result_df.at[result_df_sayac, 'Link'] = uniq_trendyol_link\n",
    "                result_df.at[result_df_sayac, 'Başlık'] = trendyol_links_main_dict[uniq_trendyol_link][0] #Başlık\n",
    "                result_df.at[result_df_sayac, 'Açıklama 1'] = trendyol_links_main_dict[uniq_trendyol_link][1] #Açıklama 1\n",
    "                result_df.at[result_df_sayac, 'Açıklama 2'] = trendyol_links_main_dict[uniq_trendyol_link][2] #Açıklama 2\n",
    "                print(kw_index, uniq_trendyol_link)\n",
    "\n",
    "                try:\n",
    "                    splitted_links = uniq_trendyol_link.split(\"-\")\n",
    "                    for index, link_piece in enumerate(splitted_links):\n",
    "                        if(len(link_piece) > 1):\n",
    "                            if((link_piece[0] == \"a\") and (link_piece[1].isdigit())):\n",
    "                                if((splitted_links[index+1][0] == \"v\") and (splitted_links[index+1][1].isdigit())):\n",
    "                                    result_df.at[result_df_sayac, 'Özel Durum swA'] =  \"swA\"\n",
    "                except:\n",
    "                    pass\n",
    "                for ozel_durum in ozel_durumlar_dict.keys():\n",
    "                    if(ozel_durum in uniq_trendyol_link):\n",
    "                        result_df.at[result_df_sayac, 'Özel Durum'] = ozel_durumlar_dict[ozel_durum]\n",
    "                result_df_sayac += 1\n",
    "\n",
    "    scraper.quit_driver()\n",
    "    print(\"100kw arattı 30dk sonra tekrar aratılacak!\")\n",
    "    time.sleep(1)\n",
    "    scraper = WebScrappingDriver()\n",
    "    scraper.get_page(link)\n",
    "\n",
    "\n",
    "    #100kw den sonra taraycıyı kapat, 30dk beklet, sonra tekrar başlat\n",
    "    if str(kw_index)[-2:] == \"50\" or str(kw_index)[-2:] == \"00\":\n",
    "        with pd.ExcelWriter(\"detaylı search output/\" + os.listdir('detaylı search input/')[0].replace(\".xlsx\",\"_output.xlsx\"), engine='xlsxwriter', engine_kwargs={'options': {'strings_to_urls': False}}) as writer:\n",
    "            result_df.to_excel(writer,sheet_name=\"sheetName\", index=False)\n",
    "\n",
    "        scraper.quit_driver()\n",
    "        print(\"100kw arattı 30dk sonra tekrar aratılacak!\")\n",
    "        time.sleep(3)\n",
    "        scraper = WebScrappingDriver()\n",
    "        scraper.get_page(link)\n",
    "\n",
    "with pd.ExcelWriter(\"detaylı search output/\" + os.listdir('detaylı search input/')[0].replace(\".xlsx\",\"_output.xlsx\"), engine='xlsxwriter', engine_kwargs={'options': {'strings_to_urls': False}}) as writer:\n",
    "    result_df.to_excel(writer,sheet_name=\"sheetName\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter(\"detaylı search output/\" + os.listdir('detaylı search input/')[0].replace(\".xlsx\",\"_output.xlsx\"), engine='xlsxwriter', engine_kwargs={'options': {'strings_to_urls': False}}) as writer:\n",
    "    result_df.to_excel(writer,sheet_name=\"sheetName\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GERİ KALAN 300 KÜSÜR BURAYA YAZILIP BAŞLATILACAK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper.quit_driver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
